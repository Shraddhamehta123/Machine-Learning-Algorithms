{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1dbdf15",
   "metadata": {},
   "source": [
    "The basic topology of such networks consists of an input layer, hidden layers and an output layer. Each layer has a certain number of nodes and these nodes are connected to the next layer’s node. And combinedly these layers and nodes get trained on given data and ultimately can be used for prediction on newly unseen data.\n",
    "\n",
    "we are going to implement this artificial neural network in Python. Our task is to build a classifier to predict the gender of the given person using ANN. The dataset used for this task consists of information on the physical parameters of the person such as nose width, long hair, forehead height, etc. So using the dataset and ANN we are going to build this binary classifier. \n",
    "\n",
    "The following steps are to be taken to implement the ANN. \n",
    "\n",
    "Step 1: load and read the data \n",
    "\n",
    "The dataset is stored in a CSV file so by using the Pandas library we are going to load this file into our system and will visualize the first five rows of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fd9f954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>long_hair</th>\n",
       "      <th>forehead_width_cm</th>\n",
       "      <th>forehead_height_cm</th>\n",
       "      <th>nose_wide</th>\n",
       "      <th>nose_long</th>\n",
       "      <th>lips_thin</th>\n",
       "      <th>distance_nose_to_lip_long</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6.3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>14.4</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.5</td>\n",
       "      <td>5.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   long_hair  forehead_width_cm  forehead_height_cm  nose_wide  nose_long  \\\n",
       "0          1               11.8                 6.1          1          0   \n",
       "1          0               14.0                 5.4          0          0   \n",
       "2          0               11.8                 6.3          1          1   \n",
       "3          0               14.4                 6.1          0          1   \n",
       "4          1               13.5                 5.9          0          0   \n",
       "\n",
       "   lips_thin  distance_nose_to_lip_long  gender  \n",
       "0          1                          1    Male  \n",
       "1          1                          0  Female  \n",
       "2          1                          1    Male  \n",
       "3          1                          1    Male  \n",
       "4          0                          0  Female  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing pandas to read data\n",
    "import pandas as pd\n",
    "# Reading the dataset\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/analyticsindiamagazine/MocksDatasets/main/gender_classification.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2c2acb",
   "metadata": {},
   "source": [
    "The above dataset consists of all the essential information that is required to classify the gender, let’s check the shape of the dataset which can give us an idea of the number of records and number of features present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53d8c3dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5001, 8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of the data\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06d6522",
   "metadata": {},
   "source": [
    "There are around 5000 records and 8 features are present in the dataset. As we are dealing with a classification problem it is necessary to check how the classes of outcome variables are spared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1467a03d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Female    2501\n",
       "Male      2500\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting labels\n",
    "data['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51338123",
   "metadata": {},
   "source": [
    "As we can see the classes are well distributed and perfectly balanced there is no need for class balancing at all. \n",
    "\n",
    "Step 2: Preprocessing and defining features\n",
    "\n",
    "The class labels are stored in textual form and we need to convert those classes into numbers because models deal with data only in form of numbers. So first we replace the Female class with 0 and the class with 1, this will ultimately realize the binary nature of the output variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "834b24f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2501\n",
       "1    2500\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lableeing with numerical values\n",
    "data['gender'] = data['gender'].replace(to_replace=['Female', 'Male'], value=[0, 1])\n",
    "data['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aa18b9",
   "metadata": {},
   "source": [
    "Above, we have successfully converted the textual form class into numbers. \n",
    "\n",
    "Next, we will define the input and output features from the dataset that we are using. The first 7 variables will be used as input variables and the last one will be used as output variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee1aff08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5001, 7)\n",
      "(5001,)\n"
     ]
    }
   ],
   "source": [
    "# Defining input and output features\n",
    "X = data.iloc[:,:-1].values\n",
    "y = data.iloc[:,-1].values\n",
    "#Now we define both input and output features, let’s check the shape of these defined data.\n",
    "# Checking shape of input-output features\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f246e3",
   "metadata": {},
   "source": [
    "Further, we will split this X and y into train and test data using scikit learn’s train_test_split method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9381bdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e79ab7e",
   "metadata": {},
   "source": [
    "Out of the original data, we are using 20% of the data for testing the data, and the rest of the data will be used to train the neural network. Below we can the shapes of both training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14316cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 7)\n",
      "(4000,)\n",
      "(1001, 7)\n",
      "(1001,)\n"
     ]
    }
   ],
   "source": [
    "# Shape of train-test sets\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077757b1",
   "metadata": {},
   "source": [
    "Step 3: Building an Artificial Neural Network\n",
    "\n",
    "To build the ANN network we used a deep learning framework called Keras with the backend of the TensorFlow library. TensorFlow is basically deep learning which facilitates all the building blocks for neural networks from basic to advanced and Keras is a wrapper library which gives us beautiful APIs to access these building blocks of TensorFlow. \n",
    "\n",
    "So will build ANN using a Keras sequential model, a model which sequentially combines the layers of the network. The Keras Dense layers are used as neurons of the networks. Now below we will first import the dependencies from the Keras library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36bb195e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\mehta\\anaconda3\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.9.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.23.4)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.56.2)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.23.5)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.41.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.3.6)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.26.7)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\mehta\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\mehta\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\mehta\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\mehta\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\mehta\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\mehta\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\mehta\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "098e8a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mehta\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Libraries for neural networks\n",
    "import tensorflow\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44966330",
   "metadata": {},
   "source": [
    "Next, we will initialize the sequential model, and add the dense layers to the model. Our model consists of 3 layers. The first input layers consist of the 12 neurons for 7 input features and these neurons will be activated according to the Relu activation function. \n",
    "\n",
    "The next hidden layer is 8 neurons with activation of Relu and the last layer will consist of only one neuron because we are going to predict the only one class from the at the time this layer will have activation function as sigmoid which is the most suitable activation function for binary classification problem. \n",
    "\n",
    "Below is how we can code our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3026054b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mehta\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Defining the neural network model\n",
    "#The sequential API allows you to create model layer by layer\n",
    "model = Sequential()\n",
    "#Dense layer means fully connected layer\n",
    "#Using ReLu as the activation function to avoid vanishing and gradient problem\n",
    "model.add(Dense(12, input_dim=7, activation='relu')) # First hiden layer\n",
    "model.add(Dense(8, activation='relu')) #Second hidden layer\n",
    "model.add(Dense(1, activation='sigmoid')) # Since this is a classificatn problem so the output has to be in terms of probabilities or values 0/1. So only 1 neuon will do the job. We use sigmoid activation functn as it gives output in the form of 0 or 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0477abb5",
   "metadata": {},
   "source": [
    "Let’s take a look at the summary of the model so we can validate whether the model has been built according to the definition or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fe2df95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 12)                96        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 209 (836.00 Byte)\n",
      "Trainable params: 209 (836.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summary of the neural network model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe6fc90",
   "metadata": {},
   "source": [
    "As we can see the model has been correctly built with the correct definition of the neuron at each layer. In the summary, the model has a total of 209 trainable parameters. \n",
    "\n",
    "Next, we have to add the loss function to the model and optimizer to optimize this loss function so that model can converge faster and at the desired point. This is usually done using the model’s .compile() method where we can specify the loss function, optimizer function, and additional certain metrics on which the model will be evaluated during training and testing.   \n",
    "\n",
    "For our problem dealing with binary classification, the binary_crossentropy will be the loss function and this loss function will be optimized using the Stochastic gradient descent optimizer. Additionally, we are also using accuracy metrics to check the scores of the metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97331f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mehta\\anaconda3\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compiling the classifier\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438ac20f",
   "metadata": {},
   "source": [
    "Now we are all set to train the model, .fit() method is used to train the model where we have to feed the training dataset that we have created in step 2, this training will take place for 10 epochs with a batch size of 10 and each epoch will have 400 instances to train the model.  \n",
    "\n",
    "The training instances for each epoch are calculated as: our training is having 4000 instances and we have chosen a batch size of 10 so for each epoch model will experience (4000/10) = 400 instances. \n",
    "\n",
    "Now let’s start the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "955d2437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1147 - accuracy: 0.9510\n",
      "Epoch 2/10\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1050 - accuracy: 0.9535\n",
      "Epoch 3/10\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1032 - accuracy: 0.9572\n",
      "Epoch 4/10\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1014 - accuracy: 0.9578\n",
      "Epoch 5/10\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1080 - accuracy: 0.9535\n",
      "Epoch 6/10\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1008 - accuracy: 0.9557\n",
      "Epoch 7/10\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1023 - accuracy: 0.9567\n",
      "Epoch 8/10\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1012 - accuracy: 0.9575\n",
      "Epoch 9/10\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1006 - accuracy: 0.9548\n",
      "Epoch 10/10\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1036 - accuracy: 0.9530\n"
     ]
    }
   ],
   "source": [
    "# Training the classifier\n",
    "history=model.fit(X_train, y_train, epochs=10, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad68fb8",
   "metadata": {},
   "source": [
    "As we can see after 10 epochs the model has reached an accuracy of 95.53% per cent on training data loss is around 0.1344 units. Let’s visualize this accuracy and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06cde40e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'History' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m], label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Performance\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'History' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(model.history['accuracy'], label = 'Accuracy')\n",
    "plt.plot(history.history['loss'], label = 'Loss')\n",
    "plt.title('Training Performance')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Performance')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d48723",
   "metadata": {},
   "source": [
    "As we can see in the above curve the loss started decreasing from 0.6 and stopped around 0.1 which is a significant drop, similarly, we can initiate the accuracy was 70% and finally, it reached around 95%. \n",
    "\n",
    "So finally let’s check the accuracy on training and testing data as below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45cf524e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 1ms/step - loss: 0.1250 - accuracy: 0.9570\n",
      "Training Accuracy: 95.70%\n",
      "\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1526 - accuracy: 0.9481\n",
      "Testing Accuracy: 94.81%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking training and test accuracies\n",
    "scores = model.evaluate(X_train, y_train)\n",
    "print (\"Training Accuracy: %.2f%%\\n\" % (scores[1]*100))\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print (\"Testing Accuracy: %.2f%%\\n\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2792f148",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a5f072f",
   "metadata": {},
   "source": [
    "As we can see the accuracy of both training and testing is above 95% which is a good sign that the model has fitted very well with training data and is able to generalize the generalizations on testing very well. \n",
    "\n",
    "So this is how we can leverage the artificial neural network to address the binary classification problem on a real-world dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23d0cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e749606e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
